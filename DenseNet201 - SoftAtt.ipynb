{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6157792c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "825d07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer,InputSpec\n",
    "from keras import layers\n",
    "import keras.layers as kl\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras import callbacks \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from  matplotlib import pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,Input,Activation,add,AveragePooling2D,BatchNormalization,Dropout\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab577f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2ef3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable\n",
    "main_dir = 'D:/Data/HAM100000 - Harvard Dataset'\n",
    "main_img_dir = main_dir + '/img_data'\n",
    "preprocessed_data_dir = main_dir + '/preprocessed_data'\n",
    "train_dir = preprocessed_data_dir + '/train'\n",
    "test_dir = preprocessed_data_dir + '/val'\n",
    "train_label = preprocessed_data_dir + '/train_label.csv'\n",
    "val_label = preprocessed_data_dir + '/val_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbaea60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_data(df_path, img_path):\n",
    "    img_list = []\n",
    "    labels = []\n",
    "    ages = []\n",
    "    sex_list = []\n",
    "    localizations = []\n",
    "    \n",
    "    df = pd.read_csv(df_path)\n",
    "    for index, row in df.iterrows():\n",
    "        img_list.append(cv2.imread(img_path + '/' + row['image_id'] + '.jpg'))\n",
    "        labels.append(row['dx'])\n",
    "        ages.append(float(row['age']))\n",
    "        sex_list.append(row['sex'])\n",
    "        localizations.append(row['localization'])\n",
    "    return (img_list, ages, sex_list, localizations), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02cc5ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      image_id   dx   age   sex localization\n",
       "0           0  ISIC_0027419  bkl  80.0  male        scalp\n",
       "1           1  ISIC_0025030  bkl  80.0  male        scalp\n",
       "2           2  ISIC_0026769  bkl  80.0  male        scalp\n",
       "3           3  ISIC_0025661  bkl  80.0  male        scalp\n",
       "4           4  ISIC_0031633  bkl  75.0  male          ear"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_label)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180af7b",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827a6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_img_lst, train_ages, train_sexes, train_localizations), labels = Get_data(train_label, main_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a06009b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(elem is None for elem in train_img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6de92f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8962\n",
      "Image shape: 450 600\n",
      "Example of Age: 80.0\n",
      "Example of Sex: male\n",
      "Example of Localization: scalp\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images: {}\".format(len(train_img_lst)))\n",
    "print(\"Image shape: {0} {1}\".format(train_img_lst[0].shape[0], train_img_lst[0].shape[1]))\n",
    "print(\"Example of Age: {}\".format(train_ages[0]))\n",
    "print(\"Example of Sex: {}\".format(train_sexes[0]))\n",
    "print(\"Example of Localization: {}\".format(train_localizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0313c",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea74922",
   "metadata": {},
   "outputs": [],
   "source": [
    "(val_img_lst, val_ages, val_sexes, val_localizations), val_labels = Get_data(val_label, main_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75851082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(elem is None for elem in train_img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc76bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 996\n",
      "Image shape: 450 600\n",
      "Example of Age: 70.0\n",
      "Example of Sex: male\n",
      "Example of Localization: back\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images: {}\".format(len(val_img_lst)))\n",
    "print(\"Image shape: {0} {1}\".format(val_img_lst[0].shape[0], val_img_lst[0].shape[1]))\n",
    "print(\"Example of Age: {}\".format(val_ages[0]))\n",
    "print(\"Example of Sex: {}\".format(val_sexes[0]))\n",
    "print(\"Example of Localization: {}\".format(val_localizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834d1c1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69815849",
   "metadata": {},
   "source": [
    "## Input Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "307f275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_input = tf.keras.Input(shape=(1,), name='age', dtype=tf.float32)\n",
    "sex_input = tf.keras.Input(shape=(1,), name='sex', dtype=tf.string)\n",
    "localizations_input = tf.keras.Input(shape=(1,), name='localization', dtype=tf.string)\n",
    "\n",
    "inputs = {'age' : age_input,\n",
    "         'sex' : sex_input,\n",
    "         'local' : localizations_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbdadbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_3')>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(train_df[['age']]))\n",
    "age_norm_input = norm(age_input)\n",
    "age_norm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83666cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'category_encoding_6')>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sex input\n",
    "sex_lookup = layers.StringLookup(vocabulary=np.unique(train_df['sex']))\n",
    "sex_one_hot = layers.CategoryEncoding(num_tokens=sex_lookup.vocabulary_size())\n",
    "\n",
    "preprocessed_sex_input = sex_lookup(sex_input)\n",
    "preprocessed_sex_input = sex_one_hot(preprocessed_sex_input)\n",
    "preprocessed_sex_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6662d963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16) dtype=float32 (created by layer 'category_encoding_7')>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Localization input\n",
    "local_lookup = layers.StringLookup(vocabulary=np.unique(train_df['localization']))\n",
    "local_one_hot = layers.CategoryEncoding(num_tokens=local_lookup.vocabulary_size())\n",
    "\n",
    "preprocess_local_input = local_lookup(localizations_input)\n",
    "preprocess_local_input = local_one_hot(preprocess_local_input)\n",
    "preprocess_local_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b5f8998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "preprocessed_inputs = [age_norm_input, preprocessed_sex_input, preprocess_local_input]\n",
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "\n",
    "preprocessed_Model = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "tf.keras.utils.plot_model(model = preprocessed_Model , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac7e2f",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7d2d9",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c75dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
